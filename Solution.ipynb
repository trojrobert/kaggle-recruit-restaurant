{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle recruit restaurant solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains my solution to the [Kaggle Recruit Restaurant Visitor Forecasting](https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting). I went for simplicity and used a single, global, model. I spent most of my time doing feature engineering. I mainly focused on extracting so-called rolling statistics.\n",
    "\n",
    "By itself the output of this notebook scores 0.512 on the private leaderboard, which is enough to be part of the top 25. I then averaged my solution with the [Surprise Me](https://www.kaggle.com/the1owl/surprise-me) public kernel to score 0.507. You could say that I only did half the work. However if the public kernel was not available I would simply have built another model and used it instead. Blending kernels to score has become ubiquitous to do well on Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "\n",
    "For obvious reasons I haven't commited the data files on GitHub. To run this kernel yourself as it is you will have to organise your files in a certain way. All the files provided by Kaggle should be stored in the `data/kaggle` directory. Additionally, the weather data located [here](https://www.kaggle.com/huntermcgushion/rrv-weather-data) has to be kept in the `data/weather` directory.\n",
    "\n",
    "First of all let's load the visit data. I did something very important in the following code block. Instead of taking the visiting data as is, I resampled it by day so that for days where there are no data points the number of visits is 0. This is important for computing rolling features based on time. I also keep track of whether a point was included in the original dataset or was added by the resampling procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>air_store_id</th>\n",
       "      <th>visit_date</th>\n",
       "      <th>visitors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>air_ba937bf13d40fb24</td>\n",
       "      <td>2016-01-13</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>air_ba937bf13d40fb24</td>\n",
       "      <td>2016-01-14</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>air_ba937bf13d40fb24</td>\n",
       "      <td>2016-01-15</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air_ba937bf13d40fb24</td>\n",
       "      <td>2016-01-16</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air_ba937bf13d40fb24</td>\n",
       "      <td>2016-01-18</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           air_store_id  visit_date  visitors\n",
       "0  air_ba937bf13d40fb24  2016-01-13        25\n",
       "1  air_ba937bf13d40fb24  2016-01-14        32\n",
       "2  air_ba937bf13d40fb24  2016-01-15        29\n",
       "3  air_ba937bf13d40fb24  2016-01-16        22\n",
       "4  air_ba937bf13d40fb24  2016-01-18         6"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "air_visit = pd.read_csv('data/kaggle/air_visit_data.csv')\n",
    "air_visit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "air_store_id    False\n",
       "visit_date      False\n",
       "visitors        False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "air_visit.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>air_store_id</th>\n",
       "      <th>visit_date</th>\n",
       "      <th>visitors</th>\n",
       "      <th>was_nil</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>air_00a91d42b08b08d9</td>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>35</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>air_00a91d42b08b08d9</td>\n",
       "      <td>2016-07-02</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>air_00a91d42b08b08d9</td>\n",
       "      <td>2016-07-03</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air_00a91d42b08b08d9</td>\n",
       "      <td>2016-07-04</td>\n",
       "      <td>20</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air_00a91d42b08b08d9</td>\n",
       "      <td>2016-07-05</td>\n",
       "      <td>25</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           air_store_id  visit_date  visitors  was_nil\n",
       "0  air_00a91d42b08b08d9  2016-07-01        35    False\n",
       "1  air_00a91d42b08b08d9  2016-07-02         9    False\n",
       "2  air_00a91d42b08b08d9  2016-07-03         0    False\n",
       "3  air_00a91d42b08b08d9  2016-07-04        20    False\n",
       "4  air_00a91d42b08b08d9  2016-07-05        25    False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "air_visit.index = pd.to_datetime(air_visit['visit_date'])\n",
    "air_visit = air_visit.groupby('air_store_id').apply(lambda g: g['visitors'].resample('1d').sum()).reset_index()\n",
    "air_visit['visit_date'] = air_visit['visit_date'].dt.strftime('%Y-%m-%d')\n",
    "air_visit['was_nil'] = air_visit['visitors'].isnull()\n",
    "air_visit['visitors'].fillna(0, inplace=True)\n",
    "\n",
    "air_visit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's take care of the calendar information. Apart from remaining the column names for practical reasons I added two features indicating if the previous or the next day is a holiday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>visit_date</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>prev_day_is_holiday</th>\n",
       "      <th>next_day_is_holiday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>Friday</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-03</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-04</td>\n",
       "      <td>Monday</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-05</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   visit_date day_of_week  is_holiday  prev_day_is_holiday  \\\n",
       "0  2016-01-01      Friday           1                  0.0   \n",
       "1  2016-01-02    Saturday           1                  1.0   \n",
       "2  2016-01-03      Sunday           1                  1.0   \n",
       "3  2016-01-04      Monday           0                  1.0   \n",
       "4  2016-01-05     Tuesday           0                  0.0   \n",
       "\n",
       "   next_day_is_holiday  \n",
       "0                  1.0  \n",
       "1                  1.0  \n",
       "2                  0.0  \n",
       "3                  0.0  \n",
       "4                  0.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_info = pd.read_csv('data/kaggle/date_info.csv')\n",
    "date_info.rename(columns={'holiday_flg': 'is_holiday', 'calendar_date': 'visit_date'}, inplace=True)\n",
    "date_info['prev_day_is_holiday'] = date_info['is_holiday'].shift().fillna(0)\n",
    "date_info['next_day_is_holiday'] = date_info['is_holiday'].shift(-1).fillna(0)\n",
    "\n",
    "date_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the store information I used the preprocessed version coming from the weather data instead of the \"official\" Kaggle version. The reason why is that the preprocessed version contains weather station data important for joining the weather features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>air_store_id</th>\n",
       "      <th>air_genre_name</th>\n",
       "      <th>air_area_name</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude_str</th>\n",
       "      <th>longitude_str</th>\n",
       "      <th>station_id</th>\n",
       "      <th>station_latitude</th>\n",
       "      <th>station_longitude</th>\n",
       "      <th>station_vincenty</th>\n",
       "      <th>station_great_circle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>air_0f0cdeee6c9bf3d7</td>\n",
       "      <td>Italian/French</td>\n",
       "      <td>Hyōgo-ken Kōbe-shi Kumoidōri</td>\n",
       "      <td>34.695124</td>\n",
       "      <td>135.197852</td>\n",
       "      <td>\"34.6951242\"</td>\n",
       "      <td>\"135.1978525\"</td>\n",
       "      <td>hyogo__kobe-kana__koube</td>\n",
       "      <td>34.696667</td>\n",
       "      <td>135.211667</td>\n",
       "      <td>1.277232</td>\n",
       "      <td>1.274882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>air_7cc17a324ae5c7dc</td>\n",
       "      <td>Italian/French</td>\n",
       "      <td>Hyōgo-ken Kōbe-shi Kumoidōri</td>\n",
       "      <td>34.695124</td>\n",
       "      <td>135.197852</td>\n",
       "      <td>\"34.6951242\"</td>\n",
       "      <td>\"135.1978525\"</td>\n",
       "      <td>hyogo__kobe-kana__koube</td>\n",
       "      <td>34.696667</td>\n",
       "      <td>135.211667</td>\n",
       "      <td>1.277232</td>\n",
       "      <td>1.274882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>air_fee8dcf4d619598e</td>\n",
       "      <td>Italian/French</td>\n",
       "      <td>Hyōgo-ken Kōbe-shi Kumoidōri</td>\n",
       "      <td>34.695124</td>\n",
       "      <td>135.197852</td>\n",
       "      <td>\"34.6951242\"</td>\n",
       "      <td>\"135.1978525\"</td>\n",
       "      <td>hyogo__kobe-kana__koube</td>\n",
       "      <td>34.696667</td>\n",
       "      <td>135.211667</td>\n",
       "      <td>1.277232</td>\n",
       "      <td>1.274882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air_a17f0778617c76e2</td>\n",
       "      <td>Italian/French</td>\n",
       "      <td>Hyōgo-ken Kōbe-shi Kumoidōri</td>\n",
       "      <td>34.695124</td>\n",
       "      <td>135.197852</td>\n",
       "      <td>\"34.6951242\"</td>\n",
       "      <td>\"135.1978525\"</td>\n",
       "      <td>hyogo__kobe-kana__koube</td>\n",
       "      <td>34.696667</td>\n",
       "      <td>135.211667</td>\n",
       "      <td>1.277232</td>\n",
       "      <td>1.274882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air_83db5aff8f50478e</td>\n",
       "      <td>Italian/French</td>\n",
       "      <td>Tōkyō-to Minato-ku Shibakōen</td>\n",
       "      <td>35.658068</td>\n",
       "      <td>139.751599</td>\n",
       "      <td>\"35.6580681\"</td>\n",
       "      <td>\"139.7515992\"</td>\n",
       "      <td>tokyo__tokyo-kana__tonokyo</td>\n",
       "      <td>35.691667</td>\n",
       "      <td>139.750000</td>\n",
       "      <td>3.730672</td>\n",
       "      <td>3.739835</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           air_store_id  air_genre_name                 air_area_name  \\\n",
       "0  air_0f0cdeee6c9bf3d7  Italian/French  Hyōgo-ken Kōbe-shi Kumoidōri   \n",
       "1  air_7cc17a324ae5c7dc  Italian/French  Hyōgo-ken Kōbe-shi Kumoidōri   \n",
       "2  air_fee8dcf4d619598e  Italian/French  Hyōgo-ken Kōbe-shi Kumoidōri   \n",
       "3  air_a17f0778617c76e2  Italian/French  Hyōgo-ken Kōbe-shi Kumoidōri   \n",
       "4  air_83db5aff8f50478e  Italian/French  Tōkyō-to Minato-ku Shibakōen   \n",
       "\n",
       "    latitude   longitude  latitude_str  longitude_str  \\\n",
       "0  34.695124  135.197852  \"34.6951242\"  \"135.1978525\"   \n",
       "1  34.695124  135.197852  \"34.6951242\"  \"135.1978525\"   \n",
       "2  34.695124  135.197852  \"34.6951242\"  \"135.1978525\"   \n",
       "3  34.695124  135.197852  \"34.6951242\"  \"135.1978525\"   \n",
       "4  35.658068  139.751599  \"35.6580681\"  \"139.7515992\"   \n",
       "\n",
       "                   station_id  station_latitude  station_longitude  \\\n",
       "0     hyogo__kobe-kana__koube         34.696667         135.211667   \n",
       "1     hyogo__kobe-kana__koube         34.696667         135.211667   \n",
       "2     hyogo__kobe-kana__koube         34.696667         135.211667   \n",
       "3     hyogo__kobe-kana__koube         34.696667         135.211667   \n",
       "4  tokyo__tokyo-kana__tonokyo         35.691667         139.750000   \n",
       "\n",
       "   station_vincenty  station_great_circle  \n",
       "0          1.277232              1.274882  \n",
       "1          1.277232              1.274882  \n",
       "2          1.277232              1.274882  \n",
       "3          1.277232              1.274882  \n",
       "4          3.730672              3.739835  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "air_store_info = pd.read_csv('data/weather/air_store_info_with_nearest_active_station.csv')\n",
    "\n",
    "air_store_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's handle the test set. Because of the timeseries nature of the competition the test split is implicitely contained in the example submission. We can extract the store id and the date from the `id` column. I also keep track of the order of the test because I'm a maniac and I want to make sure my submission is sorted in the right order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>visitors</th>\n",
       "      <th>air_store_id</th>\n",
       "      <th>visit_date</th>\n",
       "      <th>is_test</th>\n",
       "      <th>test_number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>air_00a91d42b08b08d9_2017-04-23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>air_00a91d42b08b08d9</td>\n",
       "      <td>2017-04-23</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>air_00a91d42b08b08d9_2017-04-24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>air_00a91d42b08b08d9</td>\n",
       "      <td>2017-04-24</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>air_00a91d42b08b08d9_2017-04-25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>air_00a91d42b08b08d9</td>\n",
       "      <td>2017-04-25</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air_00a91d42b08b08d9_2017-04-26</td>\n",
       "      <td>NaN</td>\n",
       "      <td>air_00a91d42b08b08d9</td>\n",
       "      <td>2017-04-26</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air_00a91d42b08b08d9_2017-04-27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>air_00a91d42b08b08d9</td>\n",
       "      <td>2017-04-27</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                id  visitors          air_store_id  \\\n",
       "0  air_00a91d42b08b08d9_2017-04-23       NaN  air_00a91d42b08b08d9   \n",
       "1  air_00a91d42b08b08d9_2017-04-24       NaN  air_00a91d42b08b08d9   \n",
       "2  air_00a91d42b08b08d9_2017-04-25       NaN  air_00a91d42b08b08d9   \n",
       "3  air_00a91d42b08b08d9_2017-04-26       NaN  air_00a91d42b08b08d9   \n",
       "4  air_00a91d42b08b08d9_2017-04-27       NaN  air_00a91d42b08b08d9   \n",
       "\n",
       "   visit_date  is_test  test_number  \n",
       "0  2017-04-23     True            0  \n",
       "1  2017-04-24     True            1  \n",
       "2  2017-04-25     True            2  \n",
       "3  2017-04-26     True            3  \n",
       "4  2017-04-27     True            4  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "submission = pd.read_csv('data/kaggle/sample_submission.csv')\n",
    "submission['air_store_id'] = submission['id'].str.slice(0, 20)\n",
    "submission['visit_date'] = submission['id'].str.slice(21)\n",
    "submission['is_test'] = True\n",
    "submission['visitors'] = np.nan\n",
    "submission['test_number'] = range(len(submission))\n",
    "\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the training set and the test set in same format we can merge them together. Merging the training and test sets is a good idea to extract features in one go. Let's also merge the full dataset with the calendar information and the store information extracted previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>air_store_id</th>\n",
       "      <th>visit_date</th>\n",
       "      <th>visitors</th>\n",
       "      <th>was_nil</th>\n",
       "      <th>is_test</th>\n",
       "      <th>test_number</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>prev_day_is_holiday</th>\n",
       "      <th>next_day_is_holiday</th>\n",
       "      <th>...</th>\n",
       "      <th>air_area_name</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude_str</th>\n",
       "      <th>longitude_str</th>\n",
       "      <th>station_id</th>\n",
       "      <th>station_latitude</th>\n",
       "      <th>station_longitude</th>\n",
       "      <th>station_vincenty</th>\n",
       "      <th>station_great_circle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>air_00a91d42b08b08d9</td>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>35.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Friday</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Tōkyō-to Chiyoda-ku Kudanminami</td>\n",
       "      <td>35.694003</td>\n",
       "      <td>139.753595</td>\n",
       "      <td>\"35.6940027\"</td>\n",
       "      <td>\"139.7535951\"</td>\n",
       "      <td>tokyo__tokyo-kana__tonokyo</td>\n",
       "      <td>35.691667</td>\n",
       "      <td>139.75</td>\n",
       "      <td>0.416011</td>\n",
       "      <td>0.415906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>air_00a91d42b08b08d9</td>\n",
       "      <td>2016-07-02</td>\n",
       "      <td>9.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Tōkyō-to Chiyoda-ku Kudanminami</td>\n",
       "      <td>35.694003</td>\n",
       "      <td>139.753595</td>\n",
       "      <td>\"35.6940027\"</td>\n",
       "      <td>\"139.7535951\"</td>\n",
       "      <td>tokyo__tokyo-kana__tonokyo</td>\n",
       "      <td>35.691667</td>\n",
       "      <td>139.75</td>\n",
       "      <td>0.416011</td>\n",
       "      <td>0.415906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>air_00a91d42b08b08d9</td>\n",
       "      <td>2016-07-03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Tōkyō-to Chiyoda-ku Kudanminami</td>\n",
       "      <td>35.694003</td>\n",
       "      <td>139.753595</td>\n",
       "      <td>\"35.6940027\"</td>\n",
       "      <td>\"139.7535951\"</td>\n",
       "      <td>tokyo__tokyo-kana__tonokyo</td>\n",
       "      <td>35.691667</td>\n",
       "      <td>139.75</td>\n",
       "      <td>0.416011</td>\n",
       "      <td>0.415906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air_00a91d42b08b08d9</td>\n",
       "      <td>2016-07-04</td>\n",
       "      <td>20.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Monday</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Tōkyō-to Chiyoda-ku Kudanminami</td>\n",
       "      <td>35.694003</td>\n",
       "      <td>139.753595</td>\n",
       "      <td>\"35.6940027\"</td>\n",
       "      <td>\"139.7535951\"</td>\n",
       "      <td>tokyo__tokyo-kana__tonokyo</td>\n",
       "      <td>35.691667</td>\n",
       "      <td>139.75</td>\n",
       "      <td>0.416011</td>\n",
       "      <td>0.415906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air_00a91d42b08b08d9</td>\n",
       "      <td>2016-07-05</td>\n",
       "      <td>25.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Tōkyō-to Chiyoda-ku Kudanminami</td>\n",
       "      <td>35.694003</td>\n",
       "      <td>139.753595</td>\n",
       "      <td>\"35.6940027\"</td>\n",
       "      <td>\"139.7535951\"</td>\n",
       "      <td>tokyo__tokyo-kana__tonokyo</td>\n",
       "      <td>35.691667</td>\n",
       "      <td>139.75</td>\n",
       "      <td>0.416011</td>\n",
       "      <td>0.415906</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           air_store_id  visit_date  visitors was_nil  is_test  test_number  \\\n",
       "0  air_00a91d42b08b08d9  2016-07-01      35.0   False    False          NaN   \n",
       "1  air_00a91d42b08b08d9  2016-07-02       9.0   False    False          NaN   \n",
       "2  air_00a91d42b08b08d9  2016-07-03       0.0   False    False          NaN   \n",
       "3  air_00a91d42b08b08d9  2016-07-04      20.0   False    False          NaN   \n",
       "4  air_00a91d42b08b08d9  2016-07-05      25.0   False    False          NaN   \n",
       "\n",
       "  day_of_week  is_holiday  prev_day_is_holiday  next_day_is_holiday  ...  \\\n",
       "0      Friday           0                  0.0                  0.0  ...   \n",
       "1    Saturday           0                  0.0                  0.0  ...   \n",
       "2      Sunday           0                  0.0                  0.0  ...   \n",
       "3      Monday           0                  0.0                  0.0  ...   \n",
       "4     Tuesday           0                  0.0                  0.0  ...   \n",
       "\n",
       "                     air_area_name   latitude   longitude  latitude_str  \\\n",
       "0  Tōkyō-to Chiyoda-ku Kudanminami  35.694003  139.753595  \"35.6940027\"   \n",
       "1  Tōkyō-to Chiyoda-ku Kudanminami  35.694003  139.753595  \"35.6940027\"   \n",
       "2  Tōkyō-to Chiyoda-ku Kudanminami  35.694003  139.753595  \"35.6940027\"   \n",
       "3  Tōkyō-to Chiyoda-ku Kudanminami  35.694003  139.753595  \"35.6940027\"   \n",
       "4  Tōkyō-to Chiyoda-ku Kudanminami  35.694003  139.753595  \"35.6940027\"   \n",
       "\n",
       "   longitude_str                  station_id station_latitude  \\\n",
       "0  \"139.7535951\"  tokyo__tokyo-kana__tonokyo        35.691667   \n",
       "1  \"139.7535951\"  tokyo__tokyo-kana__tonokyo        35.691667   \n",
       "2  \"139.7535951\"  tokyo__tokyo-kana__tonokyo        35.691667   \n",
       "3  \"139.7535951\"  tokyo__tokyo-kana__tonokyo        35.691667   \n",
       "4  \"139.7535951\"  tokyo__tokyo-kana__tonokyo        35.691667   \n",
       "\n",
       "   station_longitude  station_vincenty  station_great_circle  \n",
       "0             139.75          0.416011              0.415906  \n",
       "1             139.75          0.416011              0.415906  \n",
       "2             139.75          0.416011              0.415906  \n",
       "3             139.75          0.416011              0.415906  \n",
       "4             139.75          0.416011              0.415906  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.concat((air_visit, submission.drop('id', axis='columns')))\n",
    "data['is_test'].fillna(False, inplace=True)\n",
    "data = pd.merge(left=data, right=date_info, on='visit_date', how='left')\n",
    "data = pd.merge(left=data, right=air_store_info, on='air_store_id', how='left')\n",
    "data['visitors'] = data['visitors'].astype(float)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the messy part. The weather data is stored in individual files, one file for weather station. We also know each restaurant's closest weather station. To make things even more enjoyable there are some missing values for each weather station. Let's first start by loading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "weather_dfs = []\n",
    "\n",
    "for path in glob.glob('data/weather/1-1-16_5-31-17_Weather/1-1-16_5-31-17_Weather/*.csv'):\n",
    "    weather_df = pd.read_csv(path)\n",
    "    weather_df['station_id'] = path.split('\\\\')[-1].rstrip('.csv')\n",
    "    weather_dfs.append(weather_df)\n",
    "\n",
    "weather = pd.concat(weather_dfs, axis='rows')\n",
    "weather.rename(columns={'calendar_date': 'visit_date'}, inplace=True)\n",
    "\n",
    "weather.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After some testing and browsing the Kaggle forums it seemed only worthwhile to use the precipitation and the temperature features. I handled the missing values by replacing them with the global daily average. If I wasn't so lazy I would have taken the average of the $n$ closest stations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>visit_date</th>\n",
       "      <th>avg_temperature</th>\n",
       "      <th>precipitation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-03</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-04</td>\n",
       "      <td>8.8</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-05</td>\n",
       "      <td>8.9</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   visit_date  avg_temperature  precipitation\n",
       "0  2016-01-01              6.0            0.0\n",
       "1  2016-01-02              4.7            0.0\n",
       "2  2016-01-03              7.0            0.0\n",
       "3  2016-01-04              8.8            0.0\n",
       "4  2016-01-05              8.9            0.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means = weather.groupby('visit_date')[['avg_temperature', 'precipitation']].mean().reset_index()\n",
    "means.rename(columns={'avg_temperature': 'global_avg_temperature', 'precipitation': 'global_precipitation'}, inplace=True)\n",
    "weather = pd.merge(left=weather, right=means, on='visit_date', how='left')\n",
    "weather['avg_temperature'].fillna(weather['global_avg_temperature'], inplace=True)\n",
    "weather['precipitation'].fillna(weather['global_precipitation'], inplace=True)\n",
    "\n",
    "weather[['visit_date', 'avg_temperature', 'precipitation']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's tidy the final dataframe before extracting some juicy features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>air_store_id</th>\n",
       "      <th>visit_date</th>\n",
       "      <th>visitors</th>\n",
       "      <th>was_nil</th>\n",
       "      <th>is_test</th>\n",
       "      <th>test_number</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>prev_day_is_holiday</th>\n",
       "      <th>next_day_is_holiday</th>\n",
       "      <th>...</th>\n",
       "      <th>air_area_name</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude_str</th>\n",
       "      <th>longitude_str</th>\n",
       "      <th>station_id</th>\n",
       "      <th>station_latitude</th>\n",
       "      <th>station_longitude</th>\n",
       "      <th>station_vincenty</th>\n",
       "      <th>station_great_circle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>air_00a91d42b08b08d9</td>\n",
       "      <td>2016-07-01</td>\n",
       "      <td>35.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Friday</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Tōkyō-to Chiyoda-ku Kudanminami</td>\n",
       "      <td>35.694003</td>\n",
       "      <td>139.753595</td>\n",
       "      <td>\"35.6940027\"</td>\n",
       "      <td>\"139.7535951\"</td>\n",
       "      <td>tokyo__tokyo-kana__tonokyo</td>\n",
       "      <td>35.691667</td>\n",
       "      <td>139.75</td>\n",
       "      <td>0.416011</td>\n",
       "      <td>0.415906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>air_00a91d42b08b08d9</td>\n",
       "      <td>2016-07-02</td>\n",
       "      <td>9.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Tōkyō-to Chiyoda-ku Kudanminami</td>\n",
       "      <td>35.694003</td>\n",
       "      <td>139.753595</td>\n",
       "      <td>\"35.6940027\"</td>\n",
       "      <td>\"139.7535951\"</td>\n",
       "      <td>tokyo__tokyo-kana__tonokyo</td>\n",
       "      <td>35.691667</td>\n",
       "      <td>139.75</td>\n",
       "      <td>0.416011</td>\n",
       "      <td>0.415906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>air_00a91d42b08b08d9</td>\n",
       "      <td>2016-07-03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Tōkyō-to Chiyoda-ku Kudanminami</td>\n",
       "      <td>35.694003</td>\n",
       "      <td>139.753595</td>\n",
       "      <td>\"35.6940027\"</td>\n",
       "      <td>\"139.7535951\"</td>\n",
       "      <td>tokyo__tokyo-kana__tonokyo</td>\n",
       "      <td>35.691667</td>\n",
       "      <td>139.75</td>\n",
       "      <td>0.416011</td>\n",
       "      <td>0.415906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air_00a91d42b08b08d9</td>\n",
       "      <td>2016-07-04</td>\n",
       "      <td>20.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Monday</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Tōkyō-to Chiyoda-ku Kudanminami</td>\n",
       "      <td>35.694003</td>\n",
       "      <td>139.753595</td>\n",
       "      <td>\"35.6940027\"</td>\n",
       "      <td>\"139.7535951\"</td>\n",
       "      <td>tokyo__tokyo-kana__tonokyo</td>\n",
       "      <td>35.691667</td>\n",
       "      <td>139.75</td>\n",
       "      <td>0.416011</td>\n",
       "      <td>0.415906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air_00a91d42b08b08d9</td>\n",
       "      <td>2016-07-05</td>\n",
       "      <td>25.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Tōkyō-to Chiyoda-ku Kudanminami</td>\n",
       "      <td>35.694003</td>\n",
       "      <td>139.753595</td>\n",
       "      <td>\"35.6940027\"</td>\n",
       "      <td>\"139.7535951\"</td>\n",
       "      <td>tokyo__tokyo-kana__tonokyo</td>\n",
       "      <td>35.691667</td>\n",
       "      <td>139.75</td>\n",
       "      <td>0.416011</td>\n",
       "      <td>0.415906</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           air_store_id visit_date  visitors was_nil  is_test  test_number  \\\n",
       "0  air_00a91d42b08b08d9 2016-07-01      35.0   False    False          NaN   \n",
       "1  air_00a91d42b08b08d9 2016-07-02       9.0   False    False          NaN   \n",
       "2  air_00a91d42b08b08d9 2016-07-03       0.0   False    False          NaN   \n",
       "3  air_00a91d42b08b08d9 2016-07-04      20.0   False    False          NaN   \n",
       "4  air_00a91d42b08b08d9 2016-07-05      25.0   False    False          NaN   \n",
       "\n",
       "  day_of_week  is_holiday  prev_day_is_holiday  next_day_is_holiday  ...  \\\n",
       "0      Friday           0                  0.0                  0.0  ...   \n",
       "1    Saturday           0                  0.0                  0.0  ...   \n",
       "2      Sunday           0                  0.0                  0.0  ...   \n",
       "3      Monday           0                  0.0                  0.0  ...   \n",
       "4     Tuesday           0                  0.0                  0.0  ...   \n",
       "\n",
       "                     air_area_name   latitude   longitude  latitude_str  \\\n",
       "0  Tōkyō-to Chiyoda-ku Kudanminami  35.694003  139.753595  \"35.6940027\"   \n",
       "1  Tōkyō-to Chiyoda-ku Kudanminami  35.694003  139.753595  \"35.6940027\"   \n",
       "2  Tōkyō-to Chiyoda-ku Kudanminami  35.694003  139.753595  \"35.6940027\"   \n",
       "3  Tōkyō-to Chiyoda-ku Kudanminami  35.694003  139.753595  \"35.6940027\"   \n",
       "4  Tōkyō-to Chiyoda-ku Kudanminami  35.694003  139.753595  \"35.6940027\"   \n",
       "\n",
       "   longitude_str                  station_id station_latitude  \\\n",
       "0  \"139.7535951\"  tokyo__tokyo-kana__tonokyo        35.691667   \n",
       "1  \"139.7535951\"  tokyo__tokyo-kana__tonokyo        35.691667   \n",
       "2  \"139.7535951\"  tokyo__tokyo-kana__tonokyo        35.691667   \n",
       "3  \"139.7535951\"  tokyo__tokyo-kana__tonokyo        35.691667   \n",
       "4  \"139.7535951\"  tokyo__tokyo-kana__tonokyo        35.691667   \n",
       "\n",
       "   station_longitude  station_vincenty  station_great_circle  \n",
       "0             139.75          0.416011              0.415906  \n",
       "1             139.75          0.416011              0.415906  \n",
       "2             139.75          0.416011              0.415906  \n",
       "3             139.75          0.416011              0.415906  \n",
       "4             139.75          0.416011              0.415906  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['visit_date'] = pd.to_datetime(data['visit_date'])\n",
    "#data.index = data['visit_date']\n",
    "data.sort_values(['air_store_id', 'visit_date'], inplace=True)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a few iterations I noticed that there were many outliers in the datasets. For example the number of visits around New Year is clearly not representative of the rest of the year. In this case outliers are values that are extremely high (there are no negative values in the dataset, duh). I didn't go out of my way and did something really simple. I simply defined outliers as values that were outside of a confidence interval, supposing that the visits follow a normal distribution per restaurant. The 2.4 in the following code block is simply a high quantile of the normal distribution. You could just as well have used 1.96 if you wanted to choose the top 5% of values as outliers. Once I have detected outliers, I created a new variable called `visitors_capped` where the outlier values are replaced with the maximum of the non-outlier values. You can read more about my method (and others) by checking out [this thread](https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting/discussion/46939)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_outliers(series):\n",
    "    return (series - series.mean()) > 2.4 * series.std()\n",
    "\n",
    "\n",
    "def cap_values(series):\n",
    "    outliers = find_outliers(series)\n",
    "    max_val = series[~outliers].max()\n",
    "    series[outliers] = max_val\n",
    "    return series\n",
    "\n",
    "\n",
    "stores = data.groupby('air_store_id')\n",
    "data['is_outlier'] = stores.apply(lambda g: find_outliers(g['visitors'])).values\n",
    "data['visitors_capped'] = stores.apply(lambda g: cap_values(g['visitors'])).values\n",
    "data['visitors_capped_log1p'] = np.log1p(data['visitors_capped'])\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I extracted some simple features. The day of the month feature is quite interesting because it can be seen as a proxy of when people get paid in the month, supposing they get paid on a monthly basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['is_weekend'] = data['day_of_week'].isin(['Saturday', 'Sunday']).astype(int)\n",
    "data['day_of_month'] = data['visit_date'].dt.day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following code block is probably the most important part of this notebook**. Exponentially weighted means (EWM) are a very way to capture the trend of a timeseries. However they rely on a paramater `alpha` which models the weighting of the mean. For example if `alpha` is close to 1 then recent values are more weighted then older ones. Choosing `alpha` is very important and there is no obvious way to choose it. I decided to do an optimization procedure to determine the best `alpha` for each store's timeseries, per day of the week.\n",
    "\n",
    "The optimization is done with [differential evolution](https://www.wikiwand.com/en/Differential_evolution). The main reason why I chose this method is that it ensures that the search space is limited between 0 and 1 (the domain of `alpha`). The objective function is the mean squared error between the timeseries and it's EWM. The EWM is computed on the timeseries shifted by 1 to make sure that the EWM can't \"look\" at the current value (else it won't work on the test set). As for how on the test, the values will be the same for the first week as well as the second, the third, etc. Of course this isn't ideal, but in my opinion there is not much else that can be done.\n",
    "\n",
    "One important thing is that I applied the following procedure per store and per day of the week but also per store and per weekend. The latter is less granular and is helpful for timeseries with not much information. Also we apply the procedure to the capped visitor information (calculated previously) and to it's logarithm. The reason why I'm doing this for the logarithm is that it smooths the timeseries and can help a bit. Variety is the spice of life."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "\n",
    "\n",
    "def calc_shifted_ewm(series, alpha, adjust=True):\n",
    "    return series.shift().ewm(alpha=alpha, adjust=adjust).mean()\n",
    "\n",
    "\n",
    "def find_best_signal(series, adjust=False, eps=10e-5):\n",
    "    \n",
    "    def f(alpha):\n",
    "        shifted_ewm = calc_shifted_ewm(series=series, alpha=min(max(alpha, 0), 1), adjust=adjust)\n",
    "        corr = np.mean(np.power(series - shifted_ewm, 2))\n",
    "        return corr\n",
    "     \n",
    "    res = optimize.differential_evolution(func=f, bounds=[(0 + eps, 1 - eps)])\n",
    "    \n",
    "    return calc_shifted_ewm(series=series, alpha=res['x'][0], adjust=adjust)\n",
    "\n",
    "\n",
    "roll = data.groupby(['air_store_id', 'day_of_week']).apply(lambda g: find_best_signal(g['visitors_capped']))\n",
    "data['optimized_ewm_by_air_store_id_&_day_of_week'] = roll.sort_index(level=['air_store_id', 'visit_date']).values\n",
    "\n",
    "roll = data.groupby(['air_store_id', 'is_weekend']).apply(lambda g: find_best_signal(g['visitors_capped']))\n",
    "data['optimized_ewm_by_air_store_id_&_is_weekend'] = roll.sort_index(level=['air_store_id', 'visit_date']).values\n",
    "\n",
    "roll = data.groupby(['air_store_id', 'day_of_week']).apply(lambda g: find_best_signal(g['visitors_capped_log1p']))\n",
    "data['optimized_ewm_log1p_by_air_store_id_&_day_of_week'] = roll.sort_index(level=['air_store_id', 'visit_date']).values\n",
    "\n",
    "roll = data.groupby(['air_store_id', 'is_weekend']).apply(lambda g: find_best_signal(g['visitors_capped_log1p']))\n",
    "data['optimized_ewm_log1p_by_air_store_id_&_is_weekend'] = roll.sort_index(level=['air_store_id', 'visit_date']).values\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also extracted some more \"naive\" rolling features. I calculated the mean, median, standard deviation, number of values, minimum, maximum, and EWM with fixed a `alpha` parameter per timeseries. Because it's always the same I put all the logic inside a function and made the most of pandas's `groupby` and `apply`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_precedent_statistics(df, on, group_by):\n",
    "    \n",
    "    df.sort_values(group_by + ['visit_date'], inplace=True)\n",
    "    \n",
    "    groups = df.groupby(group_by, sort=False)\n",
    "    \n",
    "    stats = {\n",
    "        'mean': [],\n",
    "        'median': [],\n",
    "        'std': [],\n",
    "        'count': [],\n",
    "        'max': [],\n",
    "        'min': []\n",
    "    }\n",
    "    \n",
    "    exp_alphas = [0.1, 0.25, 0.3, 0.5, 0.75]\n",
    "    stats.update({'exp_{}_mean'.format(alpha): [] for alpha in exp_alphas})\n",
    "    \n",
    "    for _, group in groups:\n",
    "        \n",
    "        shift = group[on].shift()\n",
    "        roll = shift.rolling(window=len(group), min_periods=1)\n",
    "        \n",
    "        stats['mean'].extend(roll.mean())\n",
    "        stats['median'].extend(roll.median())\n",
    "        stats['std'].extend(roll.std())\n",
    "        stats['count'].extend(roll.count())\n",
    "        stats['max'].extend(roll.max())\n",
    "        stats['min'].extend(roll.min())\n",
    "        \n",
    "        for alpha in exp_alphas:\n",
    "            exp = shift.ewm(alpha=alpha, adjust=False)\n",
    "            stats['exp_{}_mean'.format(alpha)].extend(exp.mean())\n",
    "    \n",
    "    suffix = '_&_'.join(group_by)\n",
    "    \n",
    "    for stat_name, values in stats.items():\n",
    "        df['{}_{}_by_{}'.format(on, stat_name, suffix)] = values\n",
    "\n",
    "\n",
    "extract_precedent_statistics(\n",
    "    df=data,\n",
    "    on='visitors_capped',\n",
    "    group_by=['air_store_id', 'day_of_week']\n",
    ")\n",
    "\n",
    "extract_precedent_statistics(\n",
    "    df=data,\n",
    "    on='visitors_capped',\n",
    "    group_by=['air_store_id', 'is_weekend']\n",
    ")\n",
    "\n",
    "extract_precedent_statistics(\n",
    "    df=data,\n",
    "    on='visitors_capped',\n",
    "    group_by=['air_store_id']\n",
    ")\n",
    "\n",
    "extract_precedent_statistics(\n",
    "    df=data,\n",
    "    on='visitors_capped_log1p',\n",
    "    group_by=['air_store_id', 'day_of_week']\n",
    ")\n",
    "\n",
    "extract_precedent_statistics(\n",
    "    df=data,\n",
    "    on='visitors_capped_log1p',\n",
    "    group_by=['air_store_id', 'is_weekend']\n",
    ")\n",
    "\n",
    "extract_precedent_statistics(\n",
    "    df=data,\n",
    "    on='visitors_capped_log1p',\n",
    "    group_by=['air_store_id']\n",
    ")\n",
    "\n",
    "data.sort_values(['air_store_id', 'visit_date']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to go with boosting in the end so the categorical variables have to be one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data, columns=['day_of_week', 'air_genre_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/test split\n",
    "\n",
    "Now let's define the training and sets so that we can train a model. A few columns in the dataset have to be dropped because they are useless. I went for predicting the log of the visitors. During prediction I also predict the log of the visitors, to get back to the original magnitude I simply apply the exponential function to the prediction. This works because $exp(log(x)) = x$. I'm pretty sure anyone serious did this during the competition. I'm not sure why this works theoretically. My intuition tells me that it helps a decision tree to pack values in a leaf because the values are \"closer\" to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['visitors_log1p'] = np.log1p(data['visitors'])\n",
    "train = data[(data['is_test'] == False) & (data['is_outlier'] == False) & (data['was_nil'] == False)]\n",
    "test = data[data['is_test']].sort_values('test_number')\n",
    "\n",
    "to_drop = ['air_store_id', 'is_test', 'test_number', 'visit_date', 'was_nil',\n",
    "           'is_outlier', 'visitors_capped', 'visitors', 'air_area_name',\n",
    "           'station_id', 'station_latitude', 'station_longitude', 'station_vincenty',\n",
    "           'station_great_circle', 'visitors_capped_log1p']\n",
    "train = train.drop(to_drop, axis='columns')\n",
    "train = train.dropna()\n",
    "test = test.drop(to_drop, axis='columns')\n",
    "\n",
    "X_train = train.drop('visitors_log1p', axis='columns')\n",
    "X_test = test.drop('visitors_log1p', axis='columns')\n",
    "y_train = train['visitors_log1p']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block just does a few sanity checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert X_train.isnull().sum().sum() == 0\n",
    "assert y_train.isnull().sum() == 0\n",
    "assert len(X_train) == len(y_train)\n",
    "assert X_test.isnull().sum().sum() == 0\n",
    "assert len(X_test) == 32019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's do the actual machine learning. I went for a LightGVM cross-validation where I train 6 models on a random sample of the dataset. The sampling is done without replacement so technically this is known as *pasting*. I like doing parameter tuning myself and I don't use grid-search techniques as I find there are too burdensome. This is pretty classic stuff and isn't the most part of my notebook. A lot of Kagglers use this setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgbm\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "model = lgbm.LGBMRegressor(\n",
    "    objective='regression',\n",
    "    max_depth=5,\n",
    "    num_leaves=5 ** 2 - 1,\n",
    "    learning_rate=0.007,\n",
    "    n_estimators=30000,\n",
    "    min_child_samples=80,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=1,\n",
    "    reg_alpha=0,\n",
    "    reg_lambda=0,\n",
    "    random_state=np.random.randint(10e6)\n",
    ")\n",
    "\n",
    "n_splits = 6\n",
    "cv = model_selection.KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "val_scores = [0] * n_splits\n",
    "\n",
    "sub = submission['id'].to_frame()\n",
    "sub['visitors'] = 0\n",
    "\n",
    "feature_importances = pd.DataFrame(index=X_train.columns)\n",
    "\n",
    "for i, (fit_idx, val_idx) in enumerate(cv.split(X_train, y_train)):\n",
    "    \n",
    "    X_fit = X_train.iloc[fit_idx]\n",
    "    y_fit = y_train.iloc[fit_idx]\n",
    "    X_val = X_train.iloc[val_idx]\n",
    "    y_val = y_train.iloc[val_idx]\n",
    "    \n",
    "    model.fit(\n",
    "        X_fit,\n",
    "        y_fit,\n",
    "        eval_set=[(X_fit, y_fit), (X_val, y_val)],\n",
    "        eval_names=('fit', 'val'),\n",
    "        eval_metric='l2',\n",
    "        early_stopping_rounds=200,\n",
    "        feature_name=X_fit.columns.tolist(),\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    val_scores[i] = np.sqrt(model.best_score_['val']['l2'])\n",
    "    sub['visitors'] += model.predict(X_test, num_iteration=model.best_iteration_)\n",
    "    feature_importances[i] = model.feature_importances_\n",
    "    \n",
    "    print('Fold {} RMSLE: {:.5f}'.format(i+1, val_scores[i]))\n",
    "    \n",
    "sub['visitors'] /= n_splits\n",
    "sub['visitors'] = np.expm1(sub['visitors'])\n",
    "\n",
    "val_mean = np.mean(val_scores)\n",
    "val_std = np.std(val_scores)\n",
    "\n",
    "print('Local RMSLE: {:.5f} (±{:.5f})'.format(val_mean, val_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's save the submission. I like annotating the submission with the local score to compare it with the leaderboard score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('submissions/lgbm_{:.5f}_{:.5f}.csv'.format(val_mean, val_std), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the feature importances so to get a feeling of what worked well or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances.sort_values(0, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all there is to it! Thanks for reading. I hope you picked up some useful stuff in this notebook. Feel free to shoot me an email at **maxhalford25@gmail.com** if you have any questions. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
